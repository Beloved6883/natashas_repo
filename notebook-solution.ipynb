{"cells":[{"source":"# Code-along 2023-11-14 Running Machine Learning Experiments in Python","metadata":{},"id":"cb3d19af-61ad-4ec1-ba03-d426c0274b8c","cell_type":"markdown"},{"source":"As the climate changes, predicting the weather becomes ever more important for businesses. Since the weather depends on a lot of different factors, we want to run a lot of experiments to determine what the best approach is to predict the weather.\n\n- In this project, we will use London Weather data sourced from [Kaggle](https://www.kaggle.com/datasets/emmanuelfwerr/london-weather-data) to try and predict the temperature.\n- The focus of this code-along is running machine learning experiments. We will first do some minor exploratory data analysis, and then use `MLflow` to run experiments on what models and what hyperparameters to use.\n- This is interesting for those of you that have already trained a machine learning model before and want to see how you can speed up the process of finding the best model.","metadata":{},"id":"21503acd-6cbf-40a4-ad05-853a5da28fb7","cell_type":"markdown"},{"source":"### Import libraries\nFirst, we'll import necessary libraries, including **MLflow**.\n\n**MLflow** is an open-source platform designed to help manage the end-to-end machine learning lifecycle. It provides a comprehensive set of tools and features to streamline the process of building, training, and deploying machine learning models. Today, we'll be using MLflow for tracking experiments, hyperparameter tuning, model performance evaluation, and comparison and analysis of multiple models.\n\nTo use MLflow, we first need to install the package, since it's not included in the workspace by default. Using the `!`, we can run a bash command to install it.","metadata":{},"id":"a6386b0a-f3a1-483d-8718-fd4d86737480","cell_type":"markdown"},{"source":"!pip install mlflow","metadata":{"executionCancelledAt":null,"executionTime":10015,"lastExecutedAt":1699967821527,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install mlflow","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"d7e06601-e514-48ff-a6f9-ee3295314048","cell_type":"code","execution_count":1,"outputs":[]},{"source":"After the installation, we can import all the libraries, including MLflow. \n\n- `pandas` to import and read, and edit the data\n- `numpy` is used for calculations. \n- `MLflow` is the library we'll use for structuring our machine learning experiments\n- `seaborn` is used for visualizations\n- `sklearn`, scikit-learn is used for machine learning, with functions such as data preprocessing, model training and prediction","metadata":{},"id":"c1c50019-fee6-4769-a8fc-443a793beeb3","cell_type":"markdown"},{"source":"import pandas as pd\nimport numpy as np\nimport mlflow\nimport mlflow.sklearn\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","metadata":{"id":"bA5ajAmk7XH6","executionTime":17,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport numpy as np\nimport mlflow\nimport mlflow.sklearn\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","outputsMetadata":{"0":{"height":357,"type":"stream"},"1":{"height":209,"type":"dataFrame"}},"executionCancelledAt":null,"lastExecutedAt":1699967834568,"lastScheduledRunId":null},"id":"d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7","cell_type":"code","execution_count":3,"outputs":[]},{"source":"### Load data\nWe will be working with data stored in `london_weather.csv`, which contains the following columns:\n- **date** - recorded date of measurement - (**int**)\n- **cloud_cover** - cloud cover measurement in oktas - (**float**)\n- **sunshine** - sunshine measurement in hours (hrs) - (**float**)\n- **global_radiation** - irradiance measurement in Watt per square meter (W/m2) - (**float**)\n- **max_temp** - maximum temperature recorded in degrees Celsius (°C) - (**float**)\n- **mean_temp** - mean temperature in degrees Celsius (°C) - (**float**)\n- **min_temp** - minimum temperature recorded in degrees Celsius (°C) - (**float**)\n- **precipitation** - precipitation measurement in millimeters (mm) - (**float**)\n- **pressure** - pressure measurement in Pascals (Pa) - (**float**)\n- **snow_depth** - snow depth measurement in centimeters (cm) - (**float**)\n\nWe'll load the dataset using the pandas `read_csv` function.","metadata":{},"id":"6c1f2d9b-4dd8-4703-bd61-ca81d594c529","cell_type":"markdown"},{"source":"df = pd.read_csv('london_weather.csv')\ndf.head(5)","metadata":{"executionCancelledAt":null,"executionTime":454,"lastExecutedAt":1699969639704,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"df = pd.read_csv('london_weather.csv')\ndf.head(5)","outputsMetadata":{"0":{"height":209,"type":"dataFrame"}}},"id":"045916ab-26cb-4406-8038-7b5b7ed8e8f3","cell_type":"code","execution_count":31,"outputs":[]},{"source":"df.info()","metadata":{"executionCancelledAt":null,"executionTime":19,"lastExecutedAt":1699969649589,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"df.info()","outputsMetadata":{"0":{"height":357,"type":"stream"}}},"id":"d2eb5f30-69ad-41f6-b023-8e4e884cf63a","cell_type":"code","execution_count":32,"outputs":[]},{"source":"### Exploratory Data Analysis\nNow that we have loaded the dataset, let's perform some exploratory data analysis to understand the data better. This includes handling missing values, feature engineering, and visualizing the data.\n\n- Use pandas `pd.to_datetime` function to adjust the type of the date column\n- Also add the year and month \n- Calculate the number of missing values using pandas `isna()`\n- Select the relevant weather columns\n- Groupby year and month and calculate the mean of the relevant metrics\n- Lineplot the mean temperature per month using `seaborn`\n- Barplot the mean sunshine per month\n- Visualize a heatmap to show the correlation of features using seaborn's `heatmap()` function and pandas `.corr()` function.","metadata":{},"id":"89e965ff-e80e-431f-8943-88e15c9e78be","cell_type":"markdown"},{"source":"# Converting 'date' column to datetime format\ndf['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\n\n# Check missing values\ndf.isna().sum()","metadata":{"executionCancelledAt":null,"executionTime":1280,"lastExecutedAt":1699967929340,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Converting 'date' column to datetime format\ndf['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\n\n# Check missing values\nprint(df.isna().sum())\n\n# Grouping data by year and month, calculating mean of weather metrics\nweather_metrics = ['month', 'cloud_cover', 'sunshine', 'global_radiation', 'max_temp', 'mean_temp', 'min_temp', 'precipitation', 'pressure', 'snow_depth']\ndf_per_month = df.groupby(['year', 'month'], as_index = False)[weather_metrics].mean()\n\n# Visualizing data\nsns.lineplot(x=\"year\", y=\"mean_temp\", data=df_per_month, ci=None)\nsns.barplot(x='month', y='precipitation', data=df)\nsns.heatmap(df[weather_metrics].corr(), annot=True)","outputsMetadata":{"0":{"height":277,"type":"stream"}}},"id":"3f23ca95-6fed-4dc7-91d5-91b980d363e0","cell_type":"code","execution_count":8,"outputs":[]},{"source":"# Grouping data by year and month, calculating mean of weather metrics\nweather_metrics = ['month', 'cloud_cover', 'sunshine', 'global_radiation', 'max_temp', 'mean_temp', 'min_temp', 'precipitation', 'pressure', 'snow_depth']\ndf_per_month = df.groupby(['year', 'month'], as_index = False)[weather_metrics].mean()\n\n# Visualizing data\nsns.lineplot(x=\"year\", y=\"mean_temp\", data=df_per_month, ci=None)","metadata":{"executionCancelledAt":null,"executionTime":283,"lastExecutedAt":1699968534538,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Grouping data by year and month, calculating mean of weather metrics\nweather_metrics = ['month', 'cloud_cover', 'sunshine', 'global_radiation', 'max_temp', 'mean_temp', 'min_temp', 'precipitation', 'pressure', 'snow_depth']\ndf_per_month = df.groupby(['year', 'month'], as_index = False)[weather_metrics].mean()\n\n# Visualizing data\nsns.lineplot(x=\"year\", y=\"mean_temp\", data=df_per_month, ci=None)"},"id":"b89acf47-2861-45cc-b7ac-6d137baac60f","cell_type":"code","execution_count":25,"outputs":[]},{"source":"sns.barplot(x='month', y='sunshine', data=df)","metadata":{"executionCancelledAt":null,"executionTime":653,"lastExecutedAt":1699968556663,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"sns.barplot(x='month', y='sunshine', data=df)"},"id":"ef1e5499-152b-4b22-ac1f-04923b44335b","cell_type":"code","execution_count":27,"outputs":[]},{"source":"sns.heatmap(df[weather_metrics].corr(), annot=True)","metadata":{"executionCancelledAt":null,"executionTime":568,"lastExecutedAt":1699968666640,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"sns.heatmap(df[weather_metrics].corr(), annot=True)"},"id":"8a85291d-2c2e-45d8-8673-4f565eef35b4","cell_type":"code","execution_count":30,"outputs":[]},{"source":"### Process data into train and test sets\nNext, we have a function to preprocess the dataframe into train and test samples. \n- In this function we impute and scale the features. Imputing is done to fill in missing values, in this case using the mean, and scaling is used to put all features on the same scale, which often improves the performance.\n- Important to note is that we split the train and test set before we do imputation and scaling, such that there is no data leakage between train and test set.\n- Before running the function, we will also drop rows in which the temperature variable is unknown. Since we need to be able to train and test on the target variable at all times.","metadata":{},"id":"85b04bb2-aa2d-4ce9-83d5-273ca261adbf","cell_type":"markdown"},{"source":"def preprocess_df(df, feature_selection, target_var):\n    df = df.dropna(subset=[target_var])\n    X = df[feature_selection]    \n    y = df[target_var]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n    \n    imputer = SimpleImputer(strategy=\"mean\")\n    X_train = imputer.fit_transform(X_train)\n    X_test  = imputer.transform(X_test)\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    return X_train, X_test, y_train, y_test\n\nfeature_selection = ['month', 'cloud_cover', 'sunshine', 'global_radiation', 'snow_depth']\ntarget_var = 'mean_temp'\nX_train, X_test, y_train, y_test = preprocess_df(df, feature_selection, target_var)","metadata":{"executionCancelledAt":null,"executionTime":64,"lastExecutedAt":1699967983590,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def preprocess_df(df, feature_selection, target_var):\n    df = df.dropna(subset=[target_var])\n    X = df[feature_selection]    \n    y = df[target_var]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n    \n    imputer = SimpleImputer(strategy=\"mean\")\n    X_train = imputer.fit_transform(X_train)\n    X_test  = imputer.transform(X_test)\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    return X_train, X_test, y_train, y_test\n\nfeature_selection = ['month', 'cloud_cover', 'sunshine', 'global_radiation', 'snow_depth']\ntarget_var = 'mean_temp'\nX_train, X_test, y_train, y_test = preprocess_df(df, feature_selection, target_var)","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"8584384d-2cc7-4662-8424-fd0d9919001c","cell_type":"code","execution_count":9,"outputs":[]},{"source":"### Running a machine learning experiment\nNow we will move on to the bulk of this code-along, namely, running machine learning experiments. A simple example of training a machine learning model would go as follows:\n\n\n`lreg = LinearRegression().fit(X_train, y_train)`\n\n`y_pred = lreg.predict(x_test)`\n\n`rmse = np.sqrt(mean_squared_error(y_test, y_pred))`\n\n\nWe train a linear regression model, predict the target variable and calculate the score of the model. Repeating this for different settings, and possibly another model would result in a lot of duplicate code. You could write functions to repeatedly go over different models and hyperparameters, but you can also use MLflow. MLflow allows you to run different experiments, log the experiments, and also save the model with the best result. Since we are limited to the workspaces, we focus on tracking and logging experiments in this project.","metadata":{},"id":"e31f467c-e794-4ca4-bb7a-77dec89f650b","cell_type":"markdown"},{"source":"mlflow.set_experiment('example_experiment_1')\n\nwith mlflow.start_run():\n    model = DecisionTreeRegressor()\n    model.fit(X_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate RMSE\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n\n    # Log hyperparameters and metrics\n    mlflow.set_experiment_tag(\"model\", \"decision tree\")\n    mlflow.log_metric(\"rmse\", rmse)\n\n    # Save the model\n    mlflow.sklearn.log_model(model, \"model\")","metadata":{"executionCancelledAt":null,"executionTime":3166,"lastExecutedAt":1699968259902,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"mlflow.set_experiment('example_experiment_1')\n\nwith mlflow.start_run():\n    model = DecisionTreeRegressor()\n    model.fit(X_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate RMSE\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n\n    # Log hyperparameters and metrics\n    mlflow.set_experiment_tag(\"model\", \"decision tree\")\n    mlflow.log_metric(\"rmse\", rmse)\n\n    # Save the model\n    mlflow.sklearn.log_model(model, \"model\")"},"id":"70c9f687-1da6-42df-a3d7-95cc828e1aed","cell_type":"code","execution_count":16,"outputs":[]},{"source":"### Predicting the temperature in London\nWe will now do this for the london weather dataset. We will use two different regression models, decision tree regression, and random forest regression. We will start with just using the depth as a parameter to experiment with.\n\n- Start by declaring an `EXPERIMENT_NAME` and `EXPERIMENT_ID`, and use the `create_experiment()` function of MLflow to create an experiment.\n- Then create a `for`-loop to go over different `depth` values\n- Declare a parameters dictionary that takes the depth value from the loop. Include a `random_state` to make sure we can re-run the experiment and get similar results.\n- Use MLflow's `start_run()`, include the `experiment_id` and `run_name` in the function call.\n- Fit the `DecisionTreeRegressor()` and `RandomForestRegressor()` to the training data.\n- Get the RMSE of each model and log the parameters and metrics using MLflow's `log_param()` and `log_metric()` functions. \n- Save the experiment results using MLflow `search_runs()` function.","metadata":{},"id":"bf42a03a-fdd9-453d-aaa0-14b3d7fc09c1","cell_type":"markdown"},{"source":"# Define experiment name\nEXPERIMENT_NAME = \"experiment_1\"\nEXPERIMENT_ID = mlflow.create_experiment(EXPERIMENT_NAME)\n\n# Setup for loop\nfor idx, depth in enumerate([1, 2, 5, 10, 20]):\n    parameters = {\n        'max_depth': depth,\n        'random_state': 1\n    }\n    \n    RUN_NAME = f\"run_{idx}\"\n    \n    with mlflow.start_run(experiment_id=EXPERIMENT_ID, run_name=RUN_NAME):\n        dreg = DecisionTreeRegressor(**parameters).fit(X_train, y_train)\n        rfreg = RandomForestRegressor(**parameters).fit(X_train, y_train)\n        \n        dreg_pred = dreg.predict(X_test)\n        rfreg_pred = rfreg.predict(X_test)\n\n        dreg_r2 = r2_score(y_test, dreg_pred)\n        rfreg_r2 = r2_score(y_test, rfreg_pred)\n        \n        mlflow.log_param(\"max_depth\", depth)\n                         \n        mlflow.log_metric(\"dreg_r2\", dreg_r2)\n        mlflow.log_metric(\"rfreg_r2\", rfreg_r2)\n\n# Save experiment results\nexperiment_result = mlflow.search_runs(experiment_names=[EXPERIMENT_NAME])","metadata":{"executionCancelledAt":null,"executionTime":6262,"lastExecutedAt":1699970357244,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define experiment name\nEXPERIMENT_NAME = \"experiment_13\"\nEXPERIMENT_ID = mlflow.create_experiment(EXPERIMENT_NAME)\n\n# Setup for loop\nfor idx, depth in enumerate([1, 2, 5, 10, 20]):\n    parameters = {\n        'max_depth': depth,\n        'random_state': 1\n    }\n    \n    RUN_NAME = f\"run_{idx}\"\n    \n    with mlflow.start_run(experiment_id=EXPERIMENT_ID, run_name=RUN_NAME):\n        dreg = DecisionTreeRegressor(**parameters).fit(X_train, y_train)\n        rfreg = RandomForestRegressor(**parameters).fit(X_train, y_train)\n        \n        dreg_pred = dreg.predict(X_test)\n        rfreg_pred = rfreg.predict(X_test)\n\n        dreg_r2 = r2_score(y_test, dreg_pred)\n        rfreg_r2 = r2_score(y_test, rfreg_pred)\n        \n        mlflow.log_param(\"max_depth\", depth)\n                         \n        mlflow.log_metric(\"dreg_r2\", dreg_r2)\n        mlflow.log_metric(\"rfreg_r2\", rfreg_r2)\n\n# Save experiment results\nexperiment_result = mlflow.search_runs(experiment_names=[EXPERIMENT_NAME])"},"id":"edd84fb7-2234-435a-a0d9-b86d1a7ffbc4","cell_type":"code","execution_count":34,"outputs":[]},{"source":"experiment_result","metadata":{"executionCancelledAt":null,"executionTime":38,"lastExecutedAt":1699968350467,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"\nexperiment_result","outputsMetadata":{"0":{"height":209,"type":"dataFrame"}}},"id":"88a43156-6064-4100-b2e7-a6df5195fb55","cell_type":"code","execution_count":24,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}